{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f20d516",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dask/dask/main/docs/source/images/dask_horizontal.svg\"\n",
    "     width=\"60%\"\n",
    "     alt=\"Dask logo\\\" />\n",
    "\n",
    "# Process Tabular Data with Dask DataFrame\n",
    "In this notebook we will learn about the [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html), a tabular DataFrame interface based on pandas that will automatically build parallel computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e457f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## When to use Dask DataFrames\n",
    "\n",
    "Pandas is great for tabular datasets that fit in memory. If your data fits in memory then you should use Pandas. **Dask becomes useful when the dataset you want to analyze is larger than your machine's RAM** where you would normally run into `MemoryError`s.\n",
    "\n",
    "```python\n",
    "    MemoryError:  ...\n",
    "```\n",
    "\n",
    "This also means:\n",
    "\n",
    "## Don't use Dask DataFrames if you don't need to!\n",
    "Distributed computing brings a lot of additional complexity into the mix and will **incur overhead**. If your dataset and computations fit comfortably within your local resources **this overhead will may be larger than the performance gain** you'll get by using Dask. In that case, stick with non-distributed libraries like pandas, numpy and scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b5226",
   "metadata": {
    "tags": []
   },
   "source": [
    "## About this notebook\n",
    "During this tutorial, we will work with a dataset containg NYC flight data. This dataset is only about 200MB on disk so that you can download it in a reasonable time and exercises finish quickly, but Dask Dataframes will scale to datasets much larger than the memory on your local machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61890f",
   "metadata": {},
   "source": [
    "## Getting started with Dask DataFrames\n",
    "\n",
    "Let's use Dask DataFrame's to explore the NYC flight dataset. Dask's `read_csv` function supports wildcard characters like `\"*\"` which can be used to load an entire directory of CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69dd555",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run prep_data.py -d flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3b20b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\nycflights\\\\*.csv'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files = os.path.join('data', 'nycflights', '*.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af30d10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphvizNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "     -------------------------------------- 47.1/47.1 kB 585.5 kB/s eta 0:00:00\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11b23870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a7df915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>TailNum</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>CRSElapsedTime</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>bool</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-csv, 1 graph layer</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                          Date DayOfWeek  DepTime CRSDepTime  ArrTime CRSArrTime UniqueCarrier FlightNum TailNum ActualElapsedTime CRSElapsedTime  AirTime ArrDelay DepDelay  Origin    Dest Distance   TaxiIn  TaxiOut Cancelled Diverted\n",
       "npartitions=10                                                                                                                                                                                                                            \n",
       "                datetime64[ns]     int64  float64      int64  float64      int64        object     int64  object           float64        float64  float64  float64  float64  object  object  float64  float64  float64      bool    int64\n",
       "                           ...       ...      ...        ...      ...        ...           ...       ...     ...               ...            ...      ...      ...      ...     ...     ...      ...      ...      ...       ...      ...\n",
       "...                        ...       ...      ...        ...      ...        ...           ...       ...     ...               ...            ...      ...      ...      ...     ...     ...      ...      ...      ...       ...      ...\n",
       "                           ...       ...      ...        ...      ...        ...           ...       ...     ...               ...            ...      ...      ...      ...     ...     ...      ...      ...      ...       ...      ...\n",
       "                           ...       ...      ...        ...      ...        ...           ...       ...     ...               ...            ...      ...      ...      ...     ...     ...      ...      ...      ...       ...      ...\n",
       "Dask Name: read-csv, 1 graph layer"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = dd.read_csv(files,\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={\"TailNum\": str,\n",
    "                        \"CRSElapsedTime\": float,\n",
    "                        \"Cancelled\": bool})\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61959ff",
   "metadata": {},
   "source": [
    "Notice that the representation of the dataframe object contains no data - Dask has just done enough to read the start of the first file, and infer the column names and dtypes.\n",
    "\n",
    "**Dask is lazy!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d714511c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime',\n",
       "       'UniqueCarrier', 'FlightNum', 'TailNum', 'ActualElapsedTime',\n",
       "       'CRSElapsedTime', 'AirTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest',\n",
       "       'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled', 'Diverted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a24944c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                 datetime64[ns]\n",
       "DayOfWeek                     int64\n",
       "DepTime                     float64\n",
       "CRSDepTime                    int64\n",
       "ArrTime                     float64\n",
       "CRSArrTime                    int64\n",
       "UniqueCarrier                object\n",
       "FlightNum                     int64\n",
       "TailNum                      object\n",
       "ActualElapsedTime           float64\n",
       "CRSElapsedTime              float64\n",
       "AirTime                     float64\n",
       "ArrDelay                    float64\n",
       "DepDelay                    float64\n",
       "Origin                       object\n",
       "Dest                         object\n",
       "Distance                    float64\n",
       "TaxiIn                      float64\n",
       "TaxiOut                     float64\n",
       "Cancelled                      bool\n",
       "Diverted                      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a86ccb",
   "metadata": {},
   "source": [
    "Dask DataFrames have an `.npartitions` attribute which tells you how many partitions make up a Dask DataFrame.\n",
    "\n",
    "Dask is able to process larger-than-memory datasets by cutting computations into smaller parts and processing those in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b04c8168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d2fb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The pandas Look & Feel\n",
    "Dask DataFrames implement a well-used portion of the Pandas API in a way that allows for parallel and out-of-core computation. This means that a lot of Dask DataFrame code will look and feel familiar to pandas users: \n",
    "\n",
    "```python\n",
    "import pandas as pd                   import dask.dataframe as dd\n",
    "df = pd.read_csv('2015-01-01.csv')    df = dd.read_csv('2015-*-*.csv')\n",
    "df.groupby(df.user_id).value.mean()   df.groupby(df.user_id).value.mean().compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87fe45a",
   "metadata": {
    "tags": []
   },
   "source": [
    "This is because, internally, **a Dask DataFrame is composed of many pandas DataFrames**: \n",
    "\n",
    "<img src=\"http://dask.pydata.org/en/latest/_images/dask-dataframe.svg\" width=\"50%\">\n",
    "\n",
    "Dask DataFrames are divided into different **partitions** where each partition is a pandas DataFrame. This is why driving the Dask car *can feel* like you're still driving the pandas car: Dask is performing a bunch of regular pandas operations on regular pandas objects under the hood.\n",
    "\n",
    "But don't forget that you've entered the world of distributed computing now -- which means you've added a lot more complexity to the mix. You now need to consider things like concurrency, state, data duplicates, data loss, etc.\n",
    "\n",
    "Luckily, with a high-level Collection like DataFrames, Dask handles most of these complicated questions for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6d01f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## pandas-like Computations\n",
    "\n",
    "Let's see this in action with a more involved example. Let's compute the largest flight departure delay.\n",
    "\n",
    "In pandas we could do this by iterating over each file to find the individual maximums and then find the final maximum over the individual maximums.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "files = os.listdir(os.path.join('data', 'nycflights'))\n",
    "\n",
    "maxes = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join('data', 'nycflights', file))\n",
    "    maxes.append(df.DepDelay.max())\n",
    "\n",
    "final_max = max(maxes)\n",
    "```\n",
    "\n",
    "Thankfully, we can do this with Dask DataFrames using pandas-like code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3bf7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = os.listdir(os.path.join('data', 'nycflights'))\n",
    "\n",
    "maxes = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join('data', 'nycflights', file))\n",
    "    maxes.append(df.DepDelay.max())\n",
    "\n",
    "final_max = max(maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a21dc382",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_delay = ddf[\"DepDelay\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3dce768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.Scalar<series-..., dtype=float64>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c59405",
   "metadata": {},
   "source": [
    "The above cell looks exactly like what we would do using pandas...but the result does not! \n",
    "\n",
    "Instead of the actual result of the computation, we only get some schematic information. This is because Dask DataFrames are lazily evaluated. This means that **no computation happens unless you explicitly tell Dask to do so** by calling `.compute()`.\n",
    "\n",
    "Before actually performing a computation, dask first constructs a task graph that it can use to optimize computing the result in parallel. You can think of a task graph as the recipe or routemap that contains all the necessary instructions to arrive at the final result. Once you call `.compute()`, Dask will execute the instructions contained in the task graph and perform computations in parallel.\n",
    "\n",
    "Let's look at the task graph to get a feel for how Dask's blocked algorithms work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9c5128d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\site-packages\\graphviz\\backend\\execute.py:76\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPIPE\n\u001b[1;32m---> 76\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[43m_run_input_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\site-packages\\graphviz\\backend\\execute.py:96\u001b[0m, in \u001b[0;36m_run_input_lines\u001b[1;34m(cmd, input_lines, kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_input_lines\u001b[39m(cmd, input_lines, \u001b[38;5;241m*\u001b[39m, kwargs):\n\u001b[1;32m---> 96\u001b[0m     popen \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(cmd, stdin\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m     stdin_write \u001b[38;5;241m=\u001b[39m popen\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mwrite\n",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\subprocess.py:969\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    966\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    967\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\subprocess.py:1438\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1438\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1440\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1445\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1447\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1452\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmax_delay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\site-packages\\dask\\base.py:301\u001b[0m, in \u001b[0;36mDaskMethodsMixin.visualize\u001b[1;34m(self, filename, format, optimize_graph, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmydask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, optimize_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;124;03m\"\"\"Render the computation of this object's task graph using graphviz.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m    Requires ``graphviz`` to be installed.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;124;03m    https://docs.dask.org/en/latest/optimize.html\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m visualize(\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    303\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    305\u001b[0m         optimize_graph\u001b[38;5;241m=\u001b[39moptimize_graph,\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    307\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\site-packages\\dask\\base.py:758\u001b[0m, in \u001b[0;36mvisualize\u001b[1;34m(filename, traverse, optimize_graph, maxval, engine, *args, **kwargs)\u001b[0m\n\u001b[0;32m    754\u001b[0m args, _ \u001b[38;5;241m=\u001b[39m unpack_collections(\u001b[38;5;241m*\u001b[39margs, traverse\u001b[38;5;241m=\u001b[39mtraverse)\n\u001b[0;32m    756\u001b[0m dsk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(collections_to_dsk(args, optimize_graph\u001b[38;5;241m=\u001b[39moptimize_graph))\n\u001b[1;32m--> 758\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visualize_dsk(\n\u001b[0;32m    759\u001b[0m     dsk\u001b[38;5;241m=\u001b[39mdsk,\n\u001b[0;32m    760\u001b[0m     filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    761\u001b[0m     traverse\u001b[38;5;241m=\u001b[39mtraverse,\n\u001b[0;32m    762\u001b[0m     optimize_graph\u001b[38;5;241m=\u001b[39moptimize_graph,\n\u001b[0;32m    763\u001b[0m     maxval\u001b[38;5;241m=\u001b[39mmaxval,\n\u001b[0;32m    764\u001b[0m     engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    766\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\site-packages\\dask\\base.py:893\u001b[0m, in \u001b[0;36mvisualize_dsk\u001b[1;34m(dsk, filename, traverse, optimize_graph, maxval, o, engine, limit, **kwargs)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraphviz\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dot_graph\n\u001b[1;32m--> 893\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dot_graph(dsk, filename\u001b[38;5;241m=\u001b[39mfilename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcytoscape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipycytoscape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cytoscape_graph\n",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\site-packages\\dask\\dot.py:274\u001b[0m, in \u001b[0;36mdot_graph\u001b[1;34m(dsk, filename, format, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03mRender a task graph using dot.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03mdask.dot.to_graphviz\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    273\u001b[0m g \u001b[38;5;241m=\u001b[39m to_graphviz(dsk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraphviz_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\site-packages\\dask\\dot.py:291\u001b[0m, in \u001b[0;36mgraphviz_to_file\u001b[1;34m(g, filename, format)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 291\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphviz failed to properly produce an image. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis probably means your installation of graphviz \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missues/485 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\site-packages\\graphviz\\piping.py:104\u001b[0m, in \u001b[0;36mPipe.pipe\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipe\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     56\u001b[0m          \u001b[38;5;28mformat\u001b[39m: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m          renderer: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m          engine: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     62\u001b[0m          encoding: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the source piped through the Graphviz layout command.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m        '<?xml version='\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipe_legacy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mneato_no_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneato_no_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\site-packages\\graphviz\\piping.py:121\u001b[0m, in \u001b[0;36mPipe._pipe_legacy\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@_tools\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecate_positional_args(supported_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe_legacy\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    114\u001b[0m                  \u001b[38;5;28mformat\u001b[39m: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m                  engine: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    120\u001b[0m                  encoding: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipe_future\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mneato_no_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneato_no_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\site-packages\\graphviz\\piping.py:161\u001b[0m, in \u001b[0;36mPipe._pipe_future\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m raw\u001b[38;5;241m.\u001b[39mdecode(encoding)\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_lines(\u001b[38;5;241m*\u001b[39margs, input_encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\site-packages\\graphviz\\backend\\piping.py:161\u001b[0m, in \u001b[0;36mpipe_lines\u001b[1;34m(engine, format, input_lines, input_encoding, renderer, formatter, neato_no_op, quiet)\u001b[0m\n\u001b[0;32m    155\u001b[0m cmd \u001b[38;5;241m=\u001b[39m dot_command\u001b[38;5;241m.\u001b[39mcommand(engine, \u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    156\u001b[0m                           renderer\u001b[38;5;241m=\u001b[39mrenderer,\n\u001b[0;32m    157\u001b[0m                           formatter\u001b[38;5;241m=\u001b[39mformatter,\n\u001b[0;32m    158\u001b[0m                           neato_no_op\u001b[38;5;241m=\u001b[39mneato_no_op)\n\u001b[0;32m    159\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_lines\u001b[39m\u001b[38;5;124m'\u001b[39m: (line\u001b[38;5;241m.\u001b[39mencode(input_encoding) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m input_lines)}\n\u001b[1;32m--> 161\u001b[0m proc \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mrun_check(cmd, capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, quiet\u001b[38;5;241m=\u001b[39mquiet, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstdout\n",
      "File \u001b[1;32mc:\\Users\\mmuha\\Anaconda3\\envs\\pygmt\\lib\\site-packages\\graphviz\\backend\\execute.py:81\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "max_delay.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0acae36",
   "metadata": {},
   "source": [
    "Some things to note:\n",
    "\n",
    "1.  Up until this point everything is lazy. To evaluate the result for `max_delay`, call its `compute()` method:\n",
    "2.  Dask will delete intermediate results (like the full pandas DataFrame for each file) as soon as possible.\n",
    "    -  This lets us handle datasets that are larger than memory\n",
    "    -  This means that repeated computations will have to load all of the data in each time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb3578",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "max_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05540de7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## More Dask DataFrame computations\n",
    "\n",
    "Let's see couple of examples on how the API for Dask DataFrames is the same than Pandas. If you are comfortable with Pandas, the following operations will look very familiar, except we will need to add the `compute()` to get the results wanted.\n",
    "\n",
    "### Example 1: Total of non-cancelled flights taken\n",
    "\n",
    "Notice that there is a column in our DataFrame called `\"Cancelled\"` that is a boolean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883849a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(~ddf[\"Cancelled\"]).sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df23079",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example 2: Total of non-cancelled flights taken by airport\n",
    "\n",
    "We should select the non-canceled flights, use the operation `groupby` on the `\"Origin\"` column and finally use `count` to get the detailed per airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b01ecd4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf[~ddf[\"Cancelled\"]].groupby(\"Origin\")[\"Origin\"].count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dec6b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 1: What is the average departure delay from each airport?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5668216",
   "metadata": {},
   "source": [
    "Uncomment and run the cell below to see the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319550d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf.groupby(\"Origin\")[\"DepDelay\"].mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c37b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 2: What day of the week has the worst average departure delay?\n",
    "Uncomment and run the cell below to see the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dacd560",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf.groupby(\"DayOfWeek\")[\"DepDelay\"].mean().idxmax().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da1c43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Working with Partitions\n",
    "Dask DataFrames implements a large portion of the pandas API by simply performing pandas methods on its partitions (which are pandas objects). \n",
    "\n",
    "### Mapping Functions with `map_partitions`\n",
    "However, sometimes you might want to manipulate your Dask DataFrame with a custom function. You can use the `map_partitions` method for this.\n",
    "\n",
    "Imagine you find out that there was a 2-minute error in the `DepDelay` column.\n",
    "\n",
    "Let's create a pandas `apply` function that will subtract 2 from every input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca97e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_2(df):\n",
    "    return df.apply(lambda x: x-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ffd7a",
   "metadata": {},
   "source": [
    "We can then map this function over all of our partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"Adjusted_DepDelay\"] = ddf[\"DepDelay\"].map_partitions(subtract_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f79e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[[\"DepDelay\", \"Adjusted_DepDelay\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45884f8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance tip: When to call .compute()\n",
    "\n",
    "In the examples and exercises above, we sometimes perform the same operation more than once (e.g. `read_csv`). Dask DataFrames hashes the arguments, allowing duplicate computations to be shared, and only computed once. You can use `dask.compute()` to merge task graphs of multiple operations.\n",
    "\n",
    "For example, let's compute the mean and standard deviation for departure delay of all non-canceled flights. Since Dask operations are lazy, those values aren't the final results until we `compute` them. They're just the recipe required to get the result.\n",
    "\n",
    "If we compute them with two calls to compute, there is no sharing of intermediate computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540952b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cancelled = ddf[~ddf[\"Cancelled\"]]\n",
    "mean_delay = non_cancelled[\"DepDelay\"].mean()\n",
    "std_delay = non_cancelled[\"DepDelay\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da852348",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean_delay_result = mean_delay.compute()\n",
    "std_delay_result = std_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff6c61e",
   "metadata": {},
   "source": [
    "Now, let's see how long it takes if we try computing `mean_delay` and `std_delay` with a single `compute()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db683a",
   "metadata": {},
   "source": [
    "Using `dask.compute` takes roughly 1/2 the time. This is because the task graphs for both results are merged when calling `dask.compute`, allowing shared operations (like `read_csv`) to only be done once instead of twice. In particular, using `dask.compute` only does the following once:\n",
    "\n",
    "- The calls to `read_csv`\n",
    "- The filter (`df[~df[\"Cancelled\"]]`)\n",
    "- The `\"DepDelay\"` column indexing\n",
    "- Some of the necessary reductions (`sum`, `count`)\n",
    "\n",
    "To see what the merged task graphs between multiple results look like (and what's shared), you can use the `dask.visualize` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc371ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b983d2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extra resources\n",
    "\n",
    "- Explore applying custom code to Dask DataFrames: [Dask Tutorial DataFrames](https://github.com/dask/dask-tutorial/blob/main/04_dataframe.ipynb)\n",
    "- [Dask DataFrame documentation](https://docs.dask.org/en/latest/dataframe.html)\n",
    "- [Dask DataFrame examples](https://examples.dask.org/dataframe.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
